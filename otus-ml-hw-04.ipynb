{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om4LPHujadRE"
   },
   "source": [
    "  # ДЗ №4. Трансформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "from math import log2, ceil\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA недоступен. Для вычислений будет использоватся CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA доступен. Имя используемого GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    _device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA недоступен. Для вычислений будет использоватся CPU.')\n",
    "\n",
    "    _device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и разделим их на тренировочный, тестовый и валидационный наборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер набора данных\n",
      "train: 141\n",
      "validation: 16\n",
      "test: 983\n"
     ]
    }
   ],
   "source": [
    "_data_train_val = pd.read_csv('./data/in_domain_train.csv')\n",
    "#_data_train_val = _data_train_val[:(int(len(_data_train_val) / 50))] #TODO remove\n",
    "\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(_data_train_val['sentence'], _data_train_val['acceptable'], test_size=0.1, random_state=123)\n",
    "\n",
    "_X_train = _X_train.to_numpy()\n",
    "_X_val = _X_val.to_numpy()\n",
    "_y_train = _y_train.to_numpy()\n",
    "_y_val = _y_val.to_numpy()\n",
    "\n",
    "_data_test = pd.read_csv('./data/in_domain_dev.csv')\n",
    "#_data_test = _data_test[:(int(len(_data_test) / 20))] #TODO remove\n",
    "\n",
    "_X_test = _data_test['sentence']\n",
    "_y_test = _data_test['acceptable']\n",
    "\n",
    "print('Размер набора данных')\n",
    "print('train:', len(_X_train))\n",
    "print('validation:', len(_X_val))\n",
    "print('test:', len(_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_y_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Для оценки качества будем использовать MCC (Matthews Correlation Coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "\n",
    "Определим несколько утлитных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuBertTokenizer():\n",
    "    '''\n",
    "    Принимает корпус текстов, токенизирует его и возвращает input_ids и attemtion_masks в виде тензоров.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # https://huggingface.co/ai-forever/ruBert-base\n",
    "        self._bert_tokenizer = BertTokenizer.from_pretrained('ai-forever/ruBert-base')\n",
    "\n",
    "    def transform(self, X):\n",
    "        #max_length = self._get_max_document_length(X, self._bert_tokenizer)\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for document in X:\n",
    "            encoded_dict = self._bert_tokenizer.encode_plus(document,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            max_length=64, #TODO\n",
    "                                                            pad_to_max_length=True,\n",
    "                                                            return_tensors='pt',\n",
    "                                                            truncation=True)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # преобразуем в тензоры\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    def _get_max_document_length(self, X, tokenizer:BertTokenizer):\n",
    "        # находим максимальную длину документа\n",
    "        max_length = 0\n",
    "        for document in X:\n",
    "            tokenized_document = tokenizer.encode(document, add_special_tokens=True)\n",
    "            max_length = max(max_length, len(tokenized_document))\n",
    "\n",
    "        # Увеличиваем длину до ближайшей степени двойки.\n",
    "        # Например, если максимальная длина документа 41, то берем 64.\n",
    "        result = pow(2, ceil(log2(max_length)))\n",
    "\n",
    "        if max_length != result:\n",
    "            print(f'Максимальная длинна документа [{max_length}] уже является степенью двойки.')\n",
    "        else:\n",
    "            print(f'Максимальная длинна документа [{max_length}]. Берем [{result}].')\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_data_loader:DataLoader, val_data_loader:DataLoader, num_of_epochs = 3):\n",
    "    model.to(_device)\n",
    "    model.train() # переводим модель в режим обучения\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        print(f'=== Эпоха {epoch + 1}/{num_of_epochs} ===')\n",
    "        print('Обучение...')\n",
    "\n",
    "        train_epoch(model, optimizer, train_data_loader)\n",
    "\n",
    "        print('Валидация...')\n",
    "        test(model, val_data_loader)\n",
    "\n",
    "def train_epoch(model, optimizer, data_loader:DataLoader):\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        # извлекаем данные из батча и перемещаем их на устройство\n",
    "        input_ids = batch[0].to(_device)\n",
    "        attention_mask = batch[1].to(_device)\n",
    "        labels = batch[2].to(_device)\n",
    "\n",
    "        # обнуляем предыдущие значения градиентов\n",
    "        model.zero_grad()\n",
    "\n",
    "        # делаем предсказание\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = pred.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # вычисляем градиент функции потерь\n",
    "        loss.backward()\n",
    "                \n",
    "        # обновляем веса модели\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Loss: {avg_loss}')\n",
    "    \n",
    "def test(model, data_loader:DataLoader):\n",
    "    model.to(_device)\n",
    "\n",
    "    model.eval() # переводим модель в режим использования\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_logits = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch[0].to(_device)\n",
    "        attention_mask = batch[1].to(_device)\n",
    "        labels = batch[2].to(_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += pred.loss.item()\n",
    "\n",
    "            batch_logits.append(pred.logits.detach().cpu().numpy())\n",
    "            batch_labels.append(labels.to('cpu').numpy())\n",
    "\n",
    "    logits = np.concatenate(batch_logits, axis=0)\n",
    "    labels = np.concatenate(batch_labels, axis=0)\n",
    "\n",
    "    print(f'Loss: {total_loss / len(data_loader)}')\n",
    "    print(f'MCC: {calculate_mcc(logits, labels)}')\n",
    "\n",
    "def calculate_mcc(logits, labels):\n",
    "    y_true = labels.flatten()\n",
    "    y_pred = np.argmax(logits, axis=1).flatten()\n",
    "    return matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_bert_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-base',\n",
    "                                                            num_labels = 2,\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем корпус и обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X, y):\n",
    "    input_ids, attention_masks = RuBertTokenizer().transform(X)\n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_dataset = TensorDataset(input_ids, attention_masks, tensor_y)\n",
    "    return DataLoader(dataset=tensor_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "_train_data_loader = prepare_dataset(_X_train, _y_train)\n",
    "_val_data_loader = prepare_dataset(_X_val, _y_val)\n",
    "_test_data_loader = prepare_dataset(_X_test, _y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Эпоха 1/1 ===\n",
      "Обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:32<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2409416556358337\n",
      "Валидация...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6967453956604004\n",
      "[1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "MCC: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_optimizer = AdamW(_bert_model.parameters())\n",
    "\n",
    "train(_bert_model, _optimizer, _train_data_loader, _val_data_loader, num_of_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:04<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6995247737053902\n",
      "[1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n",
      " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1] [1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1\n",
      " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
      " 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
      " 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
      " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
      " 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
      " 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
      " 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1] [[-0.16586448  0.10393166]\n",
      " [-0.18824637  0.17681757]\n",
      " [-0.18261406 -0.01513491]\n",
      " ...\n",
      " [ 0.06551087  0.05151729]\n",
      " [ 0.23132876 -0.08769972]\n",
      " [-0.15010944  0.14283077]]\n",
      "MCC: -0.007032644335383423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(_bert_model, _test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-/zero-shot с GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
