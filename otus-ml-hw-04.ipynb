{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om4LPHujadRE"
   },
   "source": [
    "  # ДЗ №4. Траснформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "from math import log2, ceil\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f'Доступно {torch.cuda.device_count()} GPU(s).')\n",
    "    print('Используемый GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и разделим их на тренировочный, тестовый и валидационный наборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_train_val = pd.read_csv('./data/in_domain_train.csv')\n",
    "_data_train_val = _data_train_val[:(int(len(_data_train_val) / 10))] #TODO\n",
    "\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(_data_train_val['sentence'], _data_train_val['acceptable'], test_size=0.1, random_state=123)\n",
    "\n",
    "_X_train = _X_train.to_numpy()\n",
    "_X_val = _X_val.to_numpy()\n",
    "_y_train = _y_train.to_numpy()\n",
    "_y_val = _y_val.to_numpy()\n",
    "\n",
    "_data_test = pd.read_csv('./data/in_domain_dev.csv')\n",
    "_X_test = _data_test['sentence']\n",
    "_y_test = _data_test['acceptable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707\n"
     ]
    }
   ],
   "source": [
    "#print(len(_X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_y_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Для оценки качества будем использовать MCC (Matthews Correlation Coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "\n",
    "Определим несколько утлитных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuBertTokenizer():\n",
    "    '''\n",
    "    Принимает корпус текстов, токенизирует его и возвращает input_ids и attemtion_masks в виде тензоров.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # https://huggingface.co/ai-forever/ruBert-base\n",
    "        self._bert_tokenizer = BertTokenizer.from_pretrained('ai-forever/ruBert-base')\n",
    "\n",
    "    def transform(self, X):\n",
    "        #max_length = self._get_max_document_length(X, self._bert_tokenizer)\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for document in X:\n",
    "            encoded_dict = self._bert_tokenizer.encode_plus(document,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            max_length=64, #TODO\n",
    "                                                            pad_to_max_length=True,\n",
    "                                                            return_tensors='pt',\n",
    "                                                            truncation=True)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # преобразуем в тензоры\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    def _get_max_document_length(self, X, tokenizer:BertTokenizer):\n",
    "        # находим максимальную длину документа\n",
    "        max_length = 0\n",
    "        for document in X:\n",
    "            tokenized_document = tokenizer.encode(document, add_special_tokens=True)\n",
    "            max_length = max(max_length, len(tokenized_document))\n",
    "\n",
    "        # Увеличиваем длину до ближайшей степени двойки.\n",
    "        # Например, если максимальная длина документа 41, то берем 64.\n",
    "        result = pow(2, ceil(log2(max_length)))\n",
    "\n",
    "        if max_length != result:\n",
    "            print(f'Максимальная длинна документа [{max_length}] уже является степенью двойки.')\n",
    "        else:\n",
    "            print(f'Максимальная длинна документа [{max_length}]. Берем [{result}].')\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    '''\n",
    "    Утилитный класс для обучения и валидации модели.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, optimizer, num_of_epochs:int):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._num_of_epochs = num_of_epochs\n",
    "\n",
    "    def train(self, test_data_loader:DataLoader, val_data_loader:DataLoader):\n",
    "        self._model.to(device)\n",
    "        self._model.train() # переводим модель в режим обучения\n",
    "\n",
    "        for epoch in range(self._num_of_epochs):\n",
    "            print(f'=== Эпоха {epoch} ===')\n",
    "            print('Обучение...')\n",
    "\n",
    "            self._train_epoch(epoch, test_data_loader)\n",
    "\n",
    "            print('Валидация...')\n",
    "            self.test(val_data_loader)\n",
    "\n",
    "    def _train_epoch(self, epoch:int, data_loader:DataLoader):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            # обнуляем предыдущие значения градиентов\n",
    "            self._model.zero_grad()\n",
    "\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            # делаем предсказание\n",
    "            # сравниваем предсказанные значения с истинными\n",
    "            pred = self._model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = pred.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # вычисляем градиент функции потерь\n",
    "            loss.backward()\n",
    "                \n",
    "            # обновляем веса модели\n",
    "            self._optimizer.step()\n",
    "\n",
    "        elapsed_time_sec = (time.time() - start_time) / 1000\n",
    "        \n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Средний loss: {avg_loss}')\n",
    "        print(f'Время обучения эпохи: {elapsed_time_sec}sec')\n",
    "    \n",
    "    def test(self, data_loader:DataLoader):\n",
    "        self._model.to(device)\n",
    "        self._model.eval() # переводим модель в режим использования\n",
    "\n",
    "        total_loss = 0\n",
    "        total_mcc = 0\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = self._model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                total_loss += pred.loss.item()\n",
    "                total_mcc += self._calculate_mcc(pred.logits, labels)\n",
    "                \n",
    "        dataset_size = len(data_loader)\n",
    "        avg_loss = total_loss / dataset_size\n",
    "        avg_mcc = total_mcc / dataset_size\n",
    "        print(f'Средний loss: {avg_loss}')\n",
    "        print(f'Средний MCC: {avg_mcc}')\n",
    "\n",
    "    def _calculate_mcc(self, logits_pred, y_true):\n",
    "        pred_flat = np.argmax(logits_pred, axis=1).flatten()\n",
    "        y_true_flat = y_true.flatten()\n",
    "        return matthews_corrcoef(y_true_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_bert_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-base',\n",
    "                                                            num_labels = 2,\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем корпус и обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X, y):\n",
    "    input_ids, attention_masks = RuBertTokenizer().transform(X)\n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_dataset = TensorDataset(input_ids, attention_masks, tensor_y)\n",
    "    return DataLoader(dataset=tensor_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "_train_data_loader = prepare_dataset(_X_train, _y_train)\n",
    "_val_data_loader = prepare_dataset(_X_val, _y_val)\n",
    "_test_data_loader = prepare_dataset(_X_test, _y_test)\n",
    "\n",
    "_optimizer = AdamW(_bert_model.parameters())\n",
    "_trainer = Trainer(_bert_model, _optimizer, num_of_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:03<00:00, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя ошибка: 0.8132301680743694\n",
      "Время обучения: 243 days, 21:36:05.837860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_trainer.train(_train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:46<00:00,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя ошибка: 0.6542324349284172\n",
      "Средний MCC: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_trainer.test(_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-/zero-shot с GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
