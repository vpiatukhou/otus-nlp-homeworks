{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om4LPHujadRE"
   },
   "source": [
    "  # ДЗ №4. Траснформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n",
    "from math import log2, ceil\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и разделим их на тренировочный, тестовый и валидационный наборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_train_val = pd.read_csv('./data/in_domain_train.csv')\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(_data_train_val['sentence'], _data_train_val['acceptable'], test_size=0.1, random_state=123)\n",
    "\n",
    "_X_train = _X_train.to_numpy()\n",
    "_X_val = _X_val.to_numpy()\n",
    "_y_train = _y_train.to_numpy()\n",
    "_y_val = _y_val.to_numpy()\n",
    "\n",
    "_data_test = pd.read_csv('./data/in_domain_dev.csv')\n",
    "_X_test = _data_test['sentence']\n",
    "_y_test = _data_test['acceptable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_y_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Для оценки качества будем использовать MCC (Matthews Correlation Coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "\n",
    "Определим несколько утлитных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuBertTokenizer():\n",
    "    '''\n",
    "    Принимает корпус текстов, токенизирует его и возвращает input_ids и attemtion_masks в виде тензоров.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # https://huggingface.co/ai-forever/ruBert-base\n",
    "        self._bert_tokenizer = BertTokenizer.from_pretrained('ai-forever/ruBert-base')\n",
    "\n",
    "    def transform(self, X):\n",
    "        max_length = self._get_max_document_length(X, self._bert_tokenizer)\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for document in X:\n",
    "            encoded_dict = self._bert_tokenizer.encode_plus(document,\n",
    "                                                            add_special_tokens=True,\n",
    "                                                            max_length=max_length,\n",
    "                                                            pad_to_max_length=True,\n",
    "                                                            return_tensors='pt',\n",
    "                                                            truncation=True)\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # преобразуем в тензоры\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        return input_ids, attention_masks\n",
    "    \n",
    "    def _get_max_document_length(self, X, tokenizer:BertTokenizer):\n",
    "        # находим максимальную длину документа\n",
    "        max_length = 0\n",
    "        for document in X:\n",
    "            tokenized_document = tokenizer.encode(document, add_special_tokens=True)\n",
    "            max_length = max(max_length, len(tokenized_document))\n",
    "\n",
    "        # Увеличиваем длину до ближайшей степени двойки.\n",
    "        # Например, если максимальная длина документа 41, то берем 64.\n",
    "        result = pow(2, ceil(log2(max_length)))\n",
    "\n",
    "        if max_length != result:\n",
    "            print(f'Максимальная длинна документа [{max_length}] уже является степенью двойки.')\n",
    "        else:\n",
    "            print(f'Максимальная длинна документа [{max_length}]. Берем [{result}].')\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    '''\n",
    "    Утилитный класс для обучения и валидации модели.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, optimizer, num_of_epochs:int):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._num_of_epochs = num_of_epochs\n",
    "\n",
    "    def train(self, data_loader:DataLoader):\n",
    "        self._model.train() # переводим модель в режим обучения\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(self._num_of_epochs):\n",
    "            print(f'Эпоха {epoch}...')\n",
    "            for batch in tqdm(data_loader):\n",
    "                # обнуляем предыдущие значения градиентов\n",
    "                self._model.zero_grad()\n",
    "\n",
    "                input_ids = batch[0]\n",
    "                attention_mask = batch[1]\n",
    "                labels = batch[2]\n",
    "\n",
    "                # делаем предсказание\n",
    "                # сравниваем предсказанные значения с истинными\n",
    "                pred = self._model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = pred.loss\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # вычисляем градиент функции потерь\n",
    "                loss.backward()\n",
    "                \n",
    "                # обновляем веса модели\n",
    "                self._optimizer.step()\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        avergate_loss = total_loss / len(data_loader)\n",
    "        print(f'Средняя ошибка: {avergate_loss}')\n",
    "        print(f'Время обучения: {datetime.timedelta(elapsed_time)}')\n",
    "    \n",
    "    def test(self, data_loader:DataLoader):\n",
    "        self._model.eval() # переводим модель в режим использования\n",
    "\n",
    "        total_loss = 0\n",
    "        y_pred = []\n",
    "\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch[0]\n",
    "            attention_mask = batch[1]\n",
    "            labels = batch[2]\n",
    "\n",
    "            with self._model.no_grad():\n",
    "                pred = self._model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                total_loss += pred.loss.item()\n",
    "                probabilities = softmax(pred.logits)\n",
    "                y_pred.append(0 if probabilities[0] > 0.5 else 1)\n",
    "\n",
    "        avergate_loss = total_loss / len(data_loader)\n",
    "        print(f'Средняя ошибка: {avergate_loss}')\n",
    "        print(f'MCC: {matthews_corrcoef(labels, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_bert_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-base',\n",
    "                                                            num_labels = 2,\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем корпус и обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная длинна документа [45] уже является степенью двойки.\n",
      "Максимальная длинна документа [42] уже является степенью двойки.\n",
      "Максимальная длинна документа [41] уже является степенью двойки.\n",
      "Эпоха 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/111 [00:37<33:51, 18.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m _optimizer \u001b[38;5;241m=\u001b[39m AdamW(_bert_model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     12\u001b[0m _trainer \u001b[38;5;241m=\u001b[39m Trainer(_bert_model, _optimizer, num_of_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_train_data_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 20\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, data_loader)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_of_epochs):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mЭпоха \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# обнуляем предыдущие значения градиентов\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:697\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_name):\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m             \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:733\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\torch\\_ops.py:1116\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_dataset(X, y):\n",
    "    input_ids, attention_masks = RuBertTokenizer().transform(X)\n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_dataset = TensorDataset(input_ids, attention_masks, tensor_y)\n",
    "    return DataLoader(dataset=tensor_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "_train_data_loader = prepare_dataset(_X_train, _y_train)\n",
    "_val_data_loader = prepare_dataset(_X_val, _y_val)\n",
    "_test_data_loader = prepare_dataset(_X_test, _y_test)\n",
    "\n",
    "_optimizer = AdamW(_bert_model.parameters())\n",
    "_trainer = Trainer(_bert_model, _optimizer, num_of_epochs=1)\n",
    "\n",
    "_trainer.train(_train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-/zero-shot с GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
