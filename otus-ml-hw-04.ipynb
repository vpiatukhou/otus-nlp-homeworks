{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om4LPHujadRE"
   },
   "source": [
    "  # ДЗ №4. Трансформеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'RandomSampler' from 'transformers' (c:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW, BertTokenizer, RandomSampler, SequentialSampler, BertForSequenceClassification, get_linear_schedule_with_warmup\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'RandomSampler' from 'transformers' (c:\\Users\\Vasilij\\home\\projects\\git\\otus\\nlp\\otus-nlp-homeworks\\.venv\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA недоступен. Для вычислений будет использоватся CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('CUDA доступен. Имя используемого GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "    _device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA недоступен. Для вычислений будет использоватся CPU.')\n",
    "\n",
    "    _device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные и разделим их на тренировочный, тестовый и валидационный наборы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер набора данных\n",
      "train: 141\n",
      "validation: 16\n",
      "test: 983\n"
     ]
    }
   ],
   "source": [
    "_data_train_val = pd.read_csv('./data/in_domain_train.csv')\n",
    "#_data_train_val = _data_train_val[:(int(len(_data_train_val) / 50))] #TODO remove\n",
    "\n",
    "_X_train, _X_val, _y_train, _y_val = train_test_split(_data_train_val['sentence'], _data_train_val['acceptable'], test_size=0.1, random_state=123)\n",
    "\n",
    "_X_train = _X_train.to_numpy()\n",
    "_X_val = _X_val.to_numpy()\n",
    "_y_train = _y_train.to_numpy()\n",
    "_y_val = _y_val.to_numpy()\n",
    "\n",
    "_data_test = pd.read_csv('./data/in_domain_dev.csv')\n",
    "#_data_test = _data_test[:(int(len(_data_test) / 20))] #TODO remove\n",
    "\n",
    "_X_test = _data_test['sentence']\n",
    "_y_test = _data_test['acceptable']\n",
    "\n",
    "print('Размер набора данных')\n",
    "print('train:', len(_X_train))\n",
    "print('validation:', len(_X_val))\n",
    "print('test:', len(_X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_y_train.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы несбалансированы. Для оценки качества будем использовать MCC (Matthews Correlation Coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение\n",
    "\n",
    "Определим несколько утлитных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, X):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for document in X:\n",
    "        encoded_dict = tokenizer.encode_plus(document,\n",
    "                                             add_special_tokens=True,\n",
    "                                             max_length=64, #TODO\n",
    "                                             pad_to_max_length=True,\n",
    "                                             return_attention_mask = True,\n",
    "                                             return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # преобразуем в тензоры\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, learning_rate_schedule, train_data_loader:DataLoader, val_data_loader:DataLoader, num_of_epochs = 3):\n",
    "    model.to(_device)\n",
    "\n",
    "    for epoch in range(num_of_epochs):\n",
    "        print(f'=== Эпоха {epoch + 1}/{num_of_epochs} ===')\n",
    "        print('Обучение...')\n",
    "\n",
    "        train_epoch(model, optimizer, learning_rate_schedule, train_data_loader)\n",
    "\n",
    "        print('Валидация...')\n",
    "        test(model, val_data_loader)\n",
    "\n",
    "def train_epoch(model, optimizer, learning_rate_schedule, data_loader:DataLoader):\n",
    "    # переводим модель в режим обучения\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        # извлекаем данные из батча и перемещаем их на устройство\n",
    "        input_ids = batch[0].to(_device)\n",
    "        attention_mask = batch[1].to(_device)\n",
    "        labels = batch[2].to(_device)\n",
    "\n",
    "        # обнуляем предыдущие значения градиентов\n",
    "        model.zero_grad()\n",
    "\n",
    "        # делаем предсказание\n",
    "        pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = pred.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # вычисляем градиент функции потерь\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "        # обновляем веса модели\n",
    "        optimizer.step()\n",
    "\n",
    "        learning_rate_schedule.step()\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f'Loss: {avg_loss}')\n",
    "    \n",
    "def test(model, data_loader:DataLoader):\n",
    "    model.to(_device)\n",
    "\n",
    "    model.eval() # переводим модель в режим использования\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    batch_logits = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for batch in tqdm(data_loader):\n",
    "        input_ids = batch[0].to(_device)\n",
    "        attention_mask = batch[1].to(_device)\n",
    "        labels = batch[2].to(_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += pred.loss.item()\n",
    "\n",
    "            batch_logits.append(pred.logits.detach().cpu().numpy())\n",
    "            batch_labels.append(labels.to('cpu').numpy())\n",
    "\n",
    "    logits = np.concatenate(batch_logits, axis=0)\n",
    "    labels = np.concatenate(batch_labels, axis=0)\n",
    "\n",
    "    print(f'Loss: {total_loss / len(data_loader)}')\n",
    "    print(f'Accuracy: {calculate_accuracy(logits, labels)}')\n",
    "    print(f'MCC: {calculate_mcc(logits, labels)}')\n",
    "\n",
    "def calculate_accuracy(logits, labels):\n",
    "    y_true = labels.flatten()\n",
    "    y_pred = np.argmax(logits, axis=1).flatten()\n",
    "    return np.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "def calculate_mcc(logits, labels):\n",
    "    y_true = labels.flatten()\n",
    "    y_pred = np.argmax(logits, axis=1).flatten()\n",
    "    return matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "_bert_model = BertForSequenceClassification.from_pretrained('ai-forever/ruBert-base',\n",
    "                                                            num_labels = 2,\n",
    "                                                            output_attentions = False,\n",
    "                                                            output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем корпус и обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(X, y, sampler_class):\n",
    "    input_ids, attention_masks = tokenize(X)\n",
    "    tensor_y = torch.tensor(y)\n",
    "    tensor_dataset = TensorDataset(input_ids, attention_masks, tensor_y)\n",
    "    sampler = sampler_class(tensor_dataset)\n",
    "    return DataLoader(dataset=tensor_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "_train_data_loader = prepare_dataset(_X_train, _y_train, sampler_class=RandomSampler)\n",
    "_val_data_loader = prepare_dataset(_X_val, _y_val, sampler_class=RandomSampler)\n",
    "_test_data_loader = prepare_dataset(_X_test, _y_test, sampler_class=SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Эпоха 1/1 ===\n",
      "Обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:32<00:00,  6.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2409416556358337\n",
      "Валидация...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6967453956604004\n",
      "[1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "MCC: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_num_of_epochs = 3\n",
    "_num_training_steps = len(_train_data_loader) * _num_of_epochs\n",
    "\n",
    "_optimizer = AdamW(_bert_model.parameters())\n",
    "_learning_rate_scheduler = get_linear_schedule_with_warmup(_optimizer, num_warmup_steps=0, num_training_steps=_num_training_steps)\n",
    "\n",
    "train(_bert_model, _optimizer, _learning_rate_scheduler, _train_data_loader, _val_data_loader, num_of_epochs=_num_of_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:04<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6995247737053902\n",
      "[1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0\n",
      " 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 0\n",
      " 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0\n",
      " 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1 0 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0\n",
      " 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 1] [1 1 1 0 1 0 1 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 1 1\n",
      " 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 1 1 1 1\n",
      " 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 1 1 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0\n",
      " 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1\n",
      " 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0\n",
      " 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1\n",
      " 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 1\n",
      " 1 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0\n",
      " 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1\n",
      " 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1\n",
      " 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1\n",
      " 0 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1\n",
      " 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 0\n",
      " 1 1 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1\n",
      " 0 1 1 0 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
      " 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 1 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 1] [[-0.16586448  0.10393166]\n",
      " [-0.18824637  0.17681757]\n",
      " [-0.18261406 -0.01513491]\n",
      " ...\n",
      " [ 0.06551087  0.05151729]\n",
      " [ 0.23132876 -0.08769972]\n",
      " [-0.15010944  0.14283077]]\n",
      "MCC: -0.007032644335383423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(_bert_model, _test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-/zero-shot с GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
